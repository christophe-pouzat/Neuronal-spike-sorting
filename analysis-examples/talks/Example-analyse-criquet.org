#+TITLE: Un exemple concret d'analyse

#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  fr
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:nil
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:
#+LaTeX_CLASS: beamer-xetex-fr
#+BEAMER_FRAME_LEVEL: 2
#+BEAMER_HEADER_EXTRA: \usetheme{default}\usecolortheme{default}
#+BEAMER_HEADER_EXTRA: \setbeamertemplate{navigation symbols}{}
#+BEAMER_HEADER_EXTRA: \setbeamercovered{invisible}
#+BEAMER_HEADER_EXTRA: \author{{\large Christophe Pouzat} \\ \vspace{0.2cm} Mathématiques Appliquées à Paris 5 (MAP5) \\ \vspace{0.2cm} Université Paris-Descartes et CNRS UMR 8145 \\ \vspace{0.2cm} \texttt{christophe.pouzat@parisdescartes.fr} }
#+BEAMER_HEADER_EXTRA: \date{Jeudi 26 avril 2012}
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_envargs(Env Args) %4BEAMER_col(Col) %8BEAMER_extra(Extra)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 :ETC
#+EPRESENT_FRAME_LEVEL: 2
#+STARTUP: beamer

* Définitions de variables et de fonctions :noexport: 
  
#+name: setup
#+begin_src emacs-lisp :results silent :exports none
  ;; M-: flyspell-generic-check-word-predicate
    (defadvice org-mode-flyspell-verify
      (after my-org-mode-flyspell-verify activate)
      "Don't spell check src blocks."
      (setq ad-return-value
            (and ad-return-value
                 (not (org-in-src-block-p))
                 (not (member 'org-block-begin-line (text-properties-at (point))))
                 (not (member 'org-block-end-line (text-properties-at (point)))))))
  
  (unless (find "beamer-xetex-fr" org-export-latex-classes :key 'car
                :test 'equal)
    (add-to-list 'org-export-latex-classes
                 '("beamer-xetex-fr"
                   "\\documentclass[hyperref={xetex, colorlinks=true, urlcolor=blue, plainpages=false, pdfpagelabels, bookmarksnumbered}]{beamer}
                    \\usepackage{xunicode,fontspec,xltxtra}
                    \\usepackage[french]{babel}
                    \\usepackage{graphicx,longtable,url,rotating}
                    \\definecolor{lightcolor}{gray}{.55}
                    \\definecolor{shadecolor}{gray}{.95}
                    \\usepackage{minted}
                    \\newminted{common-lisp}{fontsize=\\footnotesize}
                    \\setromanfont[Mapping=text-text]{Liberation Serif}
                    \\setsansfont[Mapping=text-text]{Liberation Sans}
                    \\setmonofont[Mapping=text-text]{Liberation Mono}
                    [NO-DEFAULT-PACKAGES]
                    [EXTRA]"
  org-beamer-sectioning)))
  (add-to-list 'org-export-latex-minted-langs
  '(R "r"))  
    (setq org-export-latex-minted-options
          '(("bgcolor" "shadecolor")
            ("fontsize" "\\scriptsize")))
  (setq org-latex-to-pdf-process
        '("xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
          "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
          "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
#+end_src

#+BEGIN_LaTeX 
  % \AtBeginSection[]
  % {
  %   \begin{frame}
  %     \frametitle{Sommaire}
  %     \tableofcontents[currentsection]
  %   \end{frame}
  % }
#+END_LaTeX

#+BEGIN_SRC R :exports none :session *R*
  options(OutDec=",")
#+END_SRC

#+RESULTS:
: 0

#+name: site
: http://www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat/Data_folder/

#+name: adresse-github
: https://raw.github.com/christophe-pouzat/Neuronal-spike-sorting/master/code/sorting.R

#+name: lesNoms
#+BEGIN_SRC R :exports none :cache yes
  paste("Locust_",1:4,".dat.gz",sep="")
#+END_SRC

#+name: nbMesures
: 3e+05


* Introduction :export:

** Un exemple
Nous allons illustrer ici sur un exemple concret /et simple/ les différentes étapes du tri des potentiels d'actions :
- les données, 20 s d'enregistrement effectué 15 kHz, sont disponibles à l'adresse [[http://www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat/Data.html]] ;
- les enregistrements ont été effectués dans le lobe antennaire d'un criquet (/Schistocerca americana/) ;
- aucune stimulation n'a été appliquée pendant cette portion de l'enregistrement ;
- ce sont des enregistrements de tetrode et les données enregistrées sur les quatre sites se trouvent dans les quatre fichiers : =Locust_1.dat.gz=, =Locust_2.dat.gz=, =Locust_3.dat.gz=, =Locust_4.dat.gz=.  

** Informations supplémentaires sur les données

- les données ont été filtrées entre 300 Hz et 5 kHz lors de l'acquisition ;
- les données sont stockées dans les fichiers sous forme de « réels » représentés en double précision ;
- les fichiers ne contiennent /que les données/ (pas d'en-tête, etc) ;
- les fichiers ont été compressés avec =gzip=.
 
* Téléchargement et lecture des données dans =R= :export:

** Téléchargement des données (1)

Nous allons tout effectuer dans =R=. Nous commençons par affecter à une variable, l'adresse URL du site où se trouve les données :   
*** =site= 						       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+LaTeX: \tiny
#+name: affection-site
#+BEGIN_SRC R :exports code :cache yes :noweb yes
  site <- "<<site()>>"
#+END_SRC
#+LaTeX: \normalsize

** Téléchargement des données (2)

Nous créons ensuite un vecteur de chaînes de caractères contenant les quatre noms de fichiers de données :

*** =lesNoms=						       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:

#+name: affecte-lesNoms
#+BEGIN_SRC R :exports code :noweb yes :session *R*
  lesNoms <- <<lesNoms>>
#+END_SRC

#+RESULTS: affecte-lesNoms
| Locust_1.dat.gz |
| Locust_2.dat.gz |
| Locust_3.dat.gz |
| Locust_4.dat.gz |

** Téléchargement des données (3)

Nous « terminons le travail » avec la commande =download.file= et la commande =sapply= :
*** Téléchargement					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: telechargement
#+BEGIN_SRC R :session *R* :exports code :var site=site 
  sapply(1:4,
         function(i) {
           complet <- paste(site,
                            lesNoms[i],
                            sep="")
           download.file(complet,
                         lesNoms[i],
                         mode="wb")
         })
#+END_SRC

#+RESULTS: telechargement
| 0 |
| 0 |
| 0 |
| 0 |

** Téléchargement des données (4)
Vous pouvez alors vérifier que les fichiers se trouvent bien sur votre disque avec :
*** Vérification					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+BEGIN_SRC R :session *R* :exports code :eval never
  list.files(pattern="*.dat.gz")
#+END_SRC

** Chargement des données (1)
Nous affectons à une nouvelle variable le nombre de mesures à lire dans chaque fichier (ce n'est pas indispensable, mais c'est plus propre). Nous nous souvenons que nous avons 20 s de données échantillonées à 15 kHz soit 20 x 15^3 mesures à lire par fichier :
*** nbMesures						       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: affectation-nbMesures
#+BEGIN_SRC R :session *R* :exports code :cache yes :noweb yes
  nbMesures <- <<nbMesures()>>
#+END_SRC


** Chargement des données (2)
- nous allons devoir lire des données stockées en format binaire ; c'est-à-dire pas sous forme textuelle ce qui implique que si vous ouvrez les fichiers avec votre éditeur de texte préféré, vous ne verrez que des symboles incompréhensibles ;
- la fonction que nous allons utiliser pour cela est =readBin= (vous aurez accès à la documentation de celle-ci en tapant =?readBin= en ligne de commande) ;
- comme nos fichiers de données ont été compressés nous devons utiliser une /connexion/ (un pointeur sur un fichier ouvert) et non le nom du fichier à lire comme premier argument de la fonction ;
- si nous passions le nom d'un fichier comme premier argument, =readBin= l'ouvrirait alors comme un fichier non compressé et nous n'aurions pas les bonnes données à l'arrivée.
 
** Chargement des données (3)
- l'ouverture d'un fichier compressé ce fait avec la commande =gzfile= ;
- celle-ci retourne un objet qui peut être vu comme un pointeur sur le premier élément du fichier ouvert ;
- nous ne devons pas oublier de *refermer* le fichier lorsque nous avons fini de travailler avec.

** Chargement des données (4)
Nous chargeons les données dans l'espace de travail en nous souvenant qu'elles ont été compressées avec =gzip= :
*** Lecture des données					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: lD
#+BEGIN_SRC R :session *R* :exports code :noweb yes
  lD <- sapply(lesNoms,
               function(n) {
                 mC <- gzfile(n,open="rb")
                 x <- readBin(mC,what="double",
                              n=<<nbMesures()>>)
                 close(mC)
                 x})
  colnames(lD) <- paste("site",1:4)
#+END_SRC

#+RESULTS: lD
| site 1 |
| site 2 |
| site 3 |
| site 4 |

** Chargement des données (5)

À ce stade, si tout s'est bien passé, l'objet =lD= doit être une matrice avec =nbMesures= lignes et autant de colonnes qu'il y a d'éléments dans =lesNoms=, c'est-à-dire 4 ; ce que nous vérifions avec :
*** Vérification					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: dimension-de-lD
#+BEGIN_SRC R :session *R* :exports both :results verbatim
  dim(lD)  
#+END_SRC

*** Résultat							    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
#+RESULTS: dimension-de-lD
: 300000
: 4

* Analyse préliminaire :export:


** Un conseil
Lorsque que vous analysez des données que vous n'avez pas collectées vous mêmes, une bonne chose à faire est de générer un « [[http://en.wikipedia.org/wiki/Five-number_summary][résumé à cinq nombres]] » :
*** Résumé de =lD=					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: summary-lD
#+BEGIN_SRC R :session *R* :exports both :colnames yes
  summary(lD,digits=2)
#+END_SRC

*** Résultat							    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
#+RESULTS: summary-lD
| site 1         | site 2         | site 3         | site 4        |
|----------------+----------------+----------------+---------------|
| Min.   :-9,074 | Min.   :-8,229 | Min.   :-6,890 | Min.   :-7,35 |
| 1st Qu.:-0,371 | 1st Qu.:-0,450 | 1st Qu.:-0,530 | 1st Qu.:-0,49 |
| Median :-0,029 | Median :-0,036 | Median :-0,042 | Median :-0,04 |
| Mean   : 0,000 | Mean   : 0,000 | Mean   : 0,000 | Mean   : 0,00 |
| 3rd Qu.: 0,326 | 3rd Qu.: 0,396 | 3rd Qu.: 0,469 | 3rd Qu.: 0,43 |
| Max.   :10,626 | Max.   :11,742 | Max.   : 9,849 | Max.   :10,56 |

** Conversion des données en suite chronologique

Afin de profiter des fonctionnalités offertes par =R= pour construire des graphes de suites chronologiques, nous allons convertir notre matrice de données =lD= en une suite chronologique multivariée avec la fonction =ts= :
*** Utilisation de =ts=					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+BEGIN_SRC R :session *R* :exports code :results output silent
  lD <- ts(lD,start=0,freq=15e3)
#+END_SRC 

On peut alors utiliser la /méthode/ =plot= sur un objet de classe =mts= (/multivariate time series/) avec :
*** Utilisation de =plot.ts=				       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: plot-lD
#+BEGIN_SRC R 
  plot(lD,axes=FALSE,xlab="",main="",lwd=0.2)
#+END_SRC
    
** Visualisation des données (1)

#+caption: Les 20 secondes de données sur les 4 sites.
#+attr_latex: width=0.8\textwidth
#+label: fig:lD-complet
#+name: lD-complet
#+header: :width 1000 :height 800 :cache yes :noweb yes
#+BEGIN_SRC R :session *R* :exports results :results output graphics :file lD-complet.png
  <<plot-lD>>
#+END_SRC

#+RESULTS[0e01e179035b5055b411152fa8af4e2f66a3cca8]: lD-complet
[[file:lD-complet.png]]

** Visualisation des données (2)
Avec la fonction =window= nous pouvons facilement « zoomer » sur, par exemple, les 200 premières millisecondes de données :

*** Utilisation de =window=				       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: plot-window-lD
#+BEGIN_SRC R 
  plot(window(lD,0,0.2),
       xlab="Temps (s)",
       main="")
#+END_SRC

** Visualisation des données (3)
#+caption: Les 200 premières millisecondes de données sur les 4 sites.
#+attr_latex: width=0.8\textwidth
#+name: lD-200ms
#+header: :width 1000 :height 800 :noweb yes :cache yes
#+BEGIN_SRC R :session *R* :exports results :results output graphics :file lD-200ms.png
  <<plot-window-lD>>
#+END_SRC

#+RESULTS[e3dc8f180d42fb332ad485c8b36a5844eb2317c2]: lD-200ms
[[file:lD-200ms.png]]

** Visualisation des données (4)
- on peut se faire une bonne idée des propriétés des données avec la combinaison des fonctions =plot= et =window= ;
- il est néanmoins un peu pénible et inefficace de visualiser /une bonne partie des données/ avec une résolution permettant de voir les potentiels d'action individuels, comme sur la dernière figure, de cette façon ;
- j'ai donc créé ma propre fonction / méthode, =explore=, permettant de faire cela ;
- cette fonction appelle =plot= et =window= mais elle est interactive et permet de passer de la visualisation du segment [t,t+δ] au segment suivant [t+δ,t+2δ] en tapant simplement =return=.
** Téléchargement du fichier de fonctions (1)
- mes fonctions ne sont pas encore organisées sous forme de paquet =R= ;
- la dernière version de ce fichier peut être téléchargée depuis le dépôt [[https://github.com/][github]] dans le projet =Neuronal-spike-sorting= ;
- mais plutôt que d'effectuer le téléchargement « à  la main », nous allons le faire avec =R= ;
- nous affectons donc à la variable =github= l'=URL= du fichier de fonctions :
#+LaTeX: \tiny
#+name: affection-github
#+BEGIN_SRC R :exports code :cache yes :noweb yes :session *R*
  github <- "<<adresse-github()>>"
#+END_SRC

#+RESULTS[12672ab8c069a85ee3cfeda886c1e93202370941]: affection-github
: https://raw.github.com/christophe-pouzat/Neuronal-spike-sorting/master/code/sorting.R

#+LaTeX: \normalsize

Il manque un « .R » à la fin de l'adresse affichée ci-dessus (c'est hors cadre).

** Téléchargement du fichier de fonctions (2)
- nous allons télécharger le fichier de fonctions dans un fichier que nous nommerons =sorting.R= ;
- nous allons employer la fonction =download.file=, comme pour notre téléchargement de données ;
- il y a ici une petite subtilité due au fait que nous employons une adresse sécurisée ([[http://fr.wikipedia.org/wiki/HTTPS][https]]) ;
- nous allons devoir spécifier « =method= = "wget" » pour effectuer notre tâche ;
- comme le fichier est un fichier texte (contrairement au fichier binaire précédent) nous pouvons garder la valeur par défaut de l'argument =mode= :
#+name: telechargment-de-sorting.R
#+BEGIN_SRC R :session *R* :exports code
  download.file(github,"sorting.R",method="wget")
#+END_SRC

#+RESULTS: telechargment-de-sorting.R
: 0

** Chargement du fichier de fonctions dans l'espace de travail et utilisation de =explore=
Nous chargeons les fonctions définies dans =sorting.R= de façon tout à fait banale avec la commande =source= :
#+name: chargement-sorting.R
#+BEGIN_SRC R :session *R* :exports code
  source("sorting.R")
#+END_SRC

#+RESULTS: chargement-sorting.R

Nous pouvons maintenant appeler la fonction =explore= :
#+name: explore-lD
#+BEGIN_SRC R :session *R* exports code :eval never
  explore(lD,col=c("black","grey70"))
#+END_SRC
- j'ai changé ici les couleurs utilisées par défaut ; 
- lorsque la fonction est lancée, un graphe apparaît et =R= vous fait des propositions en ligne de commande ;
- il suffit de faire un choix puis de taper =return=.  


* Normalisation des données et détection des événements :export:

** Normalisation des événements (1)
- notre première figure (\ref{fig:lD-complet}) suggère que le rapport signal sur bruit n'est pas homogène sur l'ensemble des quatre sites ;
- il semble être moins bon sur le quatrième ;
- nous allons détecter les événements sur chacun des sites et nous ne voulons pas qu'un site contribue plus ou moins à la détection à cause d'une différence de rapport signal sur bruit ;
- en somme, nous souhaitons normaliser les données de chaque site d'enregistrement de sorte que l'écart type du bruit soit le même partout ; 
- le problème est que si nous *estimons* l'écart type du bruit « directement » en ignorant les événements, la présence de ces derniers va biaiser notre estimation ;
- mais nous ne pouvons pas retirer les événements sans les avoir détecter ;
- nous souhaitons donc appliquer une méthode d'estimation de l'écart type qui ne soit pas trop affectée par la présence des événements.

** Normalisation des événements (2)
- nous souhaitons donc une méthode [[http://en.wikipedia.org/wiki/Robust_statistics][robuste]] d'estimation de l'écart type ;
- la médiane de la valeur absolue des écarts à la médiane ([[http://en.wikipedia.org/wiki/Median_absolute_deviation][Median absolute deviation]]), =MAD= = médiane(|X_i - médiane(X_j)|), fournit ce type d'estimateur : $\hat{\sigma} \approx 1.486 \, \mathrm{MAD}$ ;
- la fonction =mad= de =R= renvoie la valeur cherchée ;
- la =MAD= de chacun des quatre sites est donc simplement obtenue avec :
#+name: lD-MAD
#+BEGIN_SRC R :session *R* :exports both :cache yes
  lD.mad <- apply(lD,2,mad)
  round(lD.mad,digits=2)
#+END_SRC  

#+RESULTS[2c42f7990852031a417da1936c50a294459d57f0]: lD-MAD
| 0.52 |
| 0.63 |
| 0.74 |
| 0.68 |

** Normalisation des événements (3)

- nous allons normaliser chaque colonne en la divisant par sa =MAD= ;
- nous allons faire cela de façon « compacte », sans écrire de boucle en utilisant les propriétés de division d'une matrice par un vecteur dans =R= – attention, nous parlons ici de propriétés propres à =R=, pas de propriétés mathématiques ;
- si =M= est une matrice et =V= est un vecteur alors pour calculer =M / V=, =R= « convertit » d'abord la matrice en vecteur *en raboutant les colonnes*, les éléments « du vecteur =M= » ou ceux de =V= sont ensuite recyclés de sorte que les deux vecteurs aient la même longueur (le même nombre d'éléments) puis la division est effectuée élément par élément avant de reconvertir le résultat en une matrice ayant les mêmes dimensions que la matrice de départ =M=..

** Normalisation des événements (4)

- comme la conversion de la matrice est effectuée en raboutant les colonnes, nous devons travailler avec la transposée de =lD= pour obtenir ce que nous voulons (prenez une petite minute pour y réfléchir) ;
- lorsqu'un objet de classe suite chronologique multivariée est transposé, la classe =mts= est perdue et un objet de classe =matrix= en résulte ;
- nous utilisons finalement les deux instructions suivantes :
#+name: normalisation-lD
#+BEGIN_SRC R :session *R* :exports code :results output silent
  lD <- t(t(lD)/lD.mad)
  lD <- ts(lD,start=0,freq=15e3)
#+END_SRC

** Normalisation des événements (5)

Nous allons comparer au moyen d'un [[http://fr.wikipedia.org/wiki/Diagramme_Quantile-Quantile][diagramme Quantile-Quantile]] la normalisation basée sur la =MAD= et celle basée sur l'estimateur classique de l'écart type.
#+LaTeX: \scriptsize
#+name: comparaison-normalisation
#+BEGIN_SRC R
  gogoQ <- qnorm(ppoints(3e5))
  plot(c(-5,5),c(-5,5),type="n",
       xlab="Quantiles théoriques",
       ylab="Quantiles empiriques")
  abline(v=c(-1,1)*1.25,lty=3)
  abline(a=0,b=1)
  colV <- c("red","grey50","blue","orange")
  invisible(sapply(1:4,
                   function(i) {
                     x <- lD[,i]
                     lines(gogoQ,sort(x),col=colV[i],lwd=2)
                     lines(gogoQ,sort(x/sd(x)),col=colV[i],lwd=2,lty=2)}))
#+END_SRC
#+LaTeX: \normalsize

** Normalisation des événements (6)
#+caption: Diagrammes Quantiles-Quantiles de comparaison de la normalisation basée sur la =MAD= (lignes continues) et de celle basée sur l'écart type (lignes brisées) pour chacun des quatre sites (1 en rouge, 2 en gris, 3 en bleu, 4 en orange). Les quantiles théoriques sont ceux d'une loi normale centrée réduite. Les lignes verticales pointillées coupent l'axe des x en -1,25 et +1,25.
#+attr_latex: width=0.8\textwidth
#+name: fig-comparaison-normalisation
#+header: :width 800 :height 800 :cache yes :noweb yes
#+BEGIN_SRC R :session *R* :exports results :results output graphics :file fig-comparaison-normalisation.png
  par(cex=2)
  <<comparaison-normalisation>>
#+END_SRC

#+RESULTS[d8a32cf466c22109a1c03742fadf6106e76a1ebb]: fig-comparaison-normalisation
[[file:fig-comparaison-normalisation.png]]

