#+TITLE: Un exemple concret d'analyse

#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  fr
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:nil
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:
#+LaTeX_CLASS: beamer-xetex-fr
#+BEAMER_FRAME_LEVEL: 2
#+BEAMER_HEADER_EXTRA: \usetheme{default}\usecolortheme{default}
#+BEAMER_HEADER_EXTRA: \setbeamertemplate{navigation symbols}{}
#+BEAMER_HEADER_EXTRA: \setbeamercovered{invisible}
#+BEAMER_HEADER_EXTRA: \author{{\large Christophe Pouzat} \\ \vspace{0.2cm} Mathématiques Appliquées à Paris 5 (MAP5) \\ \vspace{0.2cm} Université Paris-Descartes et CNRS UMR 8145 \\ \vspace{0.2cm} \texttt{christophe.pouzat@parisdescartes.fr} }
#+BEAMER_HEADER_EXTRA: \date{Jeudi 26 avril 2012}
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_envargs(Env Args) %4BEAMER_col(Col) %8BEAMER_extra(Extra)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 :ETC
#+EPRESENT_FRAME_LEVEL: 2
#+STARTUP: beamer

* Définitions de variables et de fonctions :noexport: 
  
#+name: setup
#+begin_src emacs-lisp :results silent :exports none
  ;; M-: flyspell-generic-check-word-predicate
  (defadvice org-mode-flyspell-verify
    (after my-org-mode-flyspell-verify activate)
    "Don't spell check src blocks."
    (setq ad-return-value
          (and ad-return-value
               (not (org-in-src-block-p))
               (not (member 'org-block-begin-line (text-properties-at (point))))
               (not (member 'org-block-end-line (text-properties-at (point)))))))
  
  (unless (find "beamer-xetex-fr" org-export-latex-classes :key 'car
                :test 'equal)
    (add-to-list 'org-export-latex-classes
                 '("beamer-xetex-fr"
                   "\\documentclass[hyperref={xetex, colorlinks=true, urlcolor=blue, plainpages=false, pdfpagelabels, bookmarksnumbered}]{beamer}
                    \\usepackage{xunicode,fontspec,xltxtra}
                    \\usepackage[french]{babel}
                    \\usepackage{graphicx,longtable,url,rotating}
                    \\definecolor{lightcolor}{gray}{.55}
                    \\definecolor{shadecolor}{gray}{.95}
                    \\usepackage{minted}
                    \\newminted{common-lisp}{fontsize=\\footnotesize}
                    \\setromanfont[Mapping=text-text]{Liberation Serif}
                    \\setsansfont[Mapping=text-text]{Liberation Sans}
                    \\setmonofont[Mapping=text-text]{Liberation Mono}
                    [NO-DEFAULT-PACKAGES]
                    [EXTRA]"
  org-beamer-sectioning)))
  (add-to-list 'org-export-latex-minted-langs
  '(R "r"))  
    (setq org-export-latex-minted-options
          '(("bgcolor" "shadecolor")
            ("fontsize" "\\scriptsize")))
  (setq org-latex-to-pdf-process
        '("xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
          "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"
          "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
#+end_src

#+BEGIN_LaTeX 
  % \AtBeginSection[]
  % {
  %   \begin{frame}
  %     \frametitle{Sommaire}
  %     \tableofcontents[currentsection]
  %   \end{frame}
  % }
#+END_LaTeX

#+BEGIN_SRC R :exports none :session *R*
  options(OutDec=",")
#+END_SRC

#+RESULTS:
: 0

#+name: site
: http://www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat/Data_folder/

#+name: adresse-github
: https://raw.github.com/christophe-pouzat/Neuronal-spike-sorting/master/code/sorting.R

#+name: lesNoms
#+BEGIN_SRC R :exports none :cache yes
  paste("Locust_",1:4,".dat.gz",sep="")
#+END_SRC

#+name: nbMesures
: 3e+05


* Introduction :export:

** Un exemple
Nous allons illustrer ici sur un exemple concret /et simple/ les différentes étapes du tri des potentiels d'actions :
- les données, 20 s d'enregistrement effectué 15 kHz, sont disponibles à l'adresse [[http://www.biomedicale.univ-paris5.fr/physcerv/C_Pouzat/Data.html]] ;
- les enregistrements ont été effectués dans le lobe antennaire d'un criquet (/Schistocerca americana/) ;
- aucune stimulation n'a été appliquée pendant cette portion de l'enregistrement ;
- ce sont des enregistrements de tetrode et les données enregistrées sur les quatre sites se trouvent dans les quatre fichiers : =Locust_1.dat.gz=, =Locust_2.dat.gz=, =Locust_3.dat.gz=, =Locust_4.dat.gz=.  

** Informations supplémentaires sur les données

- les données ont été filtrées entre 300 Hz et 5 kHz lors de l'acquisition ;
- les données sont stockées dans les fichiers sous forme de « réels » représentés en double précision ;
- les fichiers ne contiennent /que les données/ (pas d'en-tête, etc) ;
- les fichiers ont été compressés avec =gzip=.
 
* Téléchargement et lecture des données dans =R= :export:

** Téléchargement des données (1)

Nous allons tout effectuer dans =R=. Nous commençons par affecter à une variable, l'adresse URL du site où se trouve les données :   
*** =site= 						       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+LaTeX: \tiny
#+name: affection-site
#+BEGIN_SRC R :exports code :cache yes :noweb yes
  site <- "<<site()>>"
#+END_SRC
#+LaTeX: \normalsize

** Téléchargement des données (2)

Nous créons ensuite un vecteur de chaînes de caractères contenant les quatre noms de fichiers de données :

*** =lesNoms=						       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:

#+name: affecte-lesNoms
#+BEGIN_SRC R :exports code :noweb yes :session *R*
  lesNoms <- <<lesNoms>>
#+END_SRC

#+RESULTS: affecte-lesNoms
| Locust_1.dat.gz |
| Locust_2.dat.gz |
| Locust_3.dat.gz |
| Locust_4.dat.gz |

** Téléchargement des données (3)

Nous « terminons le travail » avec la commande =download.file= et la commande =sapply= :
*** Téléchargement					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: telechargement
#+BEGIN_SRC R :session *R* :exports code :var site=site 
  sapply(1:4,
         function(i) {
           complet <- paste(site,
                            lesNoms[i],
                            sep="")
           download.file(complet,
                         lesNoms[i],
                         mode="wb")
         })
#+END_SRC

#+RESULTS: telechargement
| 0 |
| 0 |
| 0 |
| 0 |

** Téléchargement des données (4)
Vous pouvez alors vérifier que les fichiers se trouvent bien sur votre disque avec :
*** Vérification					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+BEGIN_SRC R :session *R* :exports code :eval never
  list.files(pattern="*.dat.gz")
#+END_SRC

** Chargement des données (1)
Nous affectons à une nouvelle variable le nombre de mesures à lire dans chaque fichier (ce n'est pas indispensable, mais c'est plus propre). Nous nous souvenons que nous avons 20 s de données échantillonées à 15 kHz soit 20 x 15^3 mesures à lire par fichier :
*** nbMesures						       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: affectation-nbMesures
#+BEGIN_SRC R :session *R* :exports code :cache yes :noweb yes
  nbMesures <- <<nbMesures()>>
#+END_SRC


** Chargement des données (2)
- nous allons devoir lire des données stockées en format binaire ; c'est-à-dire pas sous forme textuelle ce qui implique que si vous ouvrez les fichiers avec votre éditeur de texte préféré, vous ne verrez que des symboles incompréhensibles ;
- la fonction que nous allons utiliser pour cela est =readBin= (vous aurez accès à la documentation de celle-ci en tapant =?readBin= en ligne de commande) ;
- comme nos fichiers de données ont été compressés nous devons utiliser une /connexion/ (un pointeur sur un fichier ouvert) et non le nom du fichier à lire comme premier argument de la fonction ;
- si nous passions le nom d'un fichier comme premier argument, =readBin= l'ouvrirait alors comme un fichier non compressé et nous n'aurions pas les bonnes données à l'arrivée.
 
** Chargement des données (3)
- l'ouverture d'un fichier compressé ce fait avec la commande =gzfile= ;
- celle-ci retourne un objet qui peut être vu comme un pointeur sur le premier élément du fichier ouvert ;
- nous ne devons pas oublier de *refermer* le fichier lorsque nous avons fini de travailler avec.

** Chargement des données (4)
Nous chargeons les données dans l'espace de travail en nous souvenant qu'elles ont été compressées avec =gzip= :
*** Lecture des données					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: lD
#+BEGIN_SRC R :session *R* :exports code :noweb yes
  lD <- sapply(lesNoms,
               function(n) {
                 mC <- gzfile(n,open="rb")
                 x <- readBin(mC,what="double",
                              n=<<nbMesures()>>)
                 close(mC)
                 x})
  colnames(lD) <- paste("site",1:4)
#+END_SRC

#+RESULTS: lD
| site 1 |
| site 2 |
| site 3 |
| site 4 |

** Chargement des données (5)

À ce stade, si tout s'est bien passé, l'objet =lD= doit être une matrice avec =nbMesures= lignes et autant de colonnes qu'il y a d'éléments dans =lesNoms=, c'est-à-dire 4 ; ce que nous vérifions avec :
*** Vérification					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: dimension-de-lD
#+BEGIN_SRC R :session *R* :exports both :results verbatim
  dim(lD)  
#+END_SRC

*** Résultat							    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
#+RESULTS: dimension-de-lD
: 300000
: 4

* Analyse préliminaire :export:


** Un conseil
Lorsque que vous analysez des données que vous n'avez pas collectées vous mêmes, une bonne chose à faire est de générer un « [[http://en.wikipedia.org/wiki/Five-number_summary][résumé à cinq nombres]] » :
*** Résumé de =lD=					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: summary-lD
#+BEGIN_SRC R :session *R* :exports both :colnames yes
  summary(lD,digits=2)
#+END_SRC

*** Résultat							    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
#+RESULTS: summary-lD
| site 1         | site 2         | site 3         | site 4        |
|----------------+----------------+----------------+---------------|
| Min.   :-9,074 | Min.   :-8,229 | Min.   :-6,890 | Min.   :-7,35 |
| 1st Qu.:-0,371 | 1st Qu.:-0,450 | 1st Qu.:-0,530 | 1st Qu.:-0,49 |
| Median :-0,029 | Median :-0,036 | Median :-0,042 | Median :-0,04 |
| Mean   : 0,000 | Mean   : 0,000 | Mean   : 0,000 | Mean   : 0,00 |
| 3rd Qu.: 0,326 | 3rd Qu.: 0,396 | 3rd Qu.: 0,469 | 3rd Qu.: 0,43 |
| Max.   :10,626 | Max.   :11,742 | Max.   : 9,849 | Max.   :10,56 |

** Conversion des données en suite chronologique

Afin de profiter des fonctionnalités offertes par =R= pour construire des graphes de suites chronologiques, nous allons convertir notre matrice de données =lD= en une suite chronologique multivariée avec la fonction =ts= :
*** Utilisation de =ts=					       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+BEGIN_SRC R :session *R* :exports code :results output silent
  lD <- ts(lD,start=0,freq=15e3)
#+END_SRC 

On peut alors utiliser la /méthode/ =plot= sur un objet de classe =mts= (/multivariate time series/) avec :
*** Utilisation de =plot.ts=				       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: plot-lD
#+BEGIN_SRC R 
  plot(lD,axes=FALSE,xlab="",main="",lwd=0.2)
#+END_SRC
    
** Visualisation des données (1)

#+caption: Les 20 secondes de données sur les 4 sites.
#+attr_latex: width=0.8\textwidth
#+label: fig:lD-complet
#+name: lD-complet
#+header: :width 1000 :height 800 :cache yes :noweb yes
#+BEGIN_SRC R :session *R* :exports results :results output graphics :file lD-complet.png
  <<plot-lD>>
#+END_SRC

#+RESULTS[0e01e179035b5055b411152fa8af4e2f66a3cca8]: lD-complet
[[file:lD-complet.png]]

** Visualisation des données (2)
Avec la fonction =window= nous pouvons facilement « zoomer » sur, par exemple, les 200 premières millisecondes de données :

*** Utilisation de =window=				       :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
#+name: plot-window-lD
#+BEGIN_SRC R 
  plot(window(lD,0,0.2),
       xlab="Temps (s)",
       main="")
#+END_SRC

** Visualisation des données (3)
#+caption: Les 200 premières millisecondes de données sur les 4 sites.
#+attr_latex: width=0.8\textwidth
#+name: lD-200ms
#+header: :width 1000 :height 800 :noweb yes :cache yes
#+BEGIN_SRC R :session *R* :exports results :results output graphics :file lD-200ms.png
  <<plot-window-lD>>
#+END_SRC

#+RESULTS[e3dc8f180d42fb332ad485c8b36a5844eb2317c2]: lD-200ms
[[file:lD-200ms.png]]

** Visualisation des données (4)
- on peut se faire une bonne idée des propriétés des données avec la combinaison des fonctions =plot= et =window= ;
- il est néanmoins un peu pénible et inefficace de visualiser /une bonne partie des données/ avec une résolution permettant de voir les potentiels d'action individuels, comme sur la dernière figure, de cette façon ;
- j'ai donc créé ma propre fonction / méthode, =explore=, permettant de faire cela ;
- cette fonction appelle =plot= et =window= mais elle est interactive et permet de passer de la visualisation du segment [t,t+δ] au segment suivant [t+δ,t+2δ] en tapant simplement =return=.
** Téléchargement du fichier de fonctions (1)
- mes fonctions ne sont pas encore organisées sous forme de paquet =R= ;
- la dernière version de ce fichier peut être téléchargée depuis le dépôt [[https://github.com/][github]] dans le projet =Neuronal-spike-sorting= ;
- mais plutôt que d'effectuer le téléchargement « à  la main », nous allons le faire avec =R= ;
- nous affectons donc à la variable =github= l'=URL= du fichier de fonctions :
#+LaTeX: \tiny
#+name: affection-github
#+BEGIN_SRC R :exports code :cache yes :noweb yes :session *R*
  github <- "<<adresse-github()>>"
#+END_SRC

#+RESULTS[12672ab8c069a85ee3cfeda886c1e93202370941]: affection-github
: https://raw.github.com/christophe-pouzat/Neuronal-spike-sorting/master/code/sorting.R

#+LaTeX: \normalsize

Il manque un « .R » à la fin de l'adresse affichée ci-dessus (c'est hors cadre).

** Téléchargement du fichier de fonctions (2)
- nous allons télécharger le fichier de fonctions dans un fichier que nous nommerons =sorting.R= ;
- nous allons employer la fonction =download.file=, comme pour notre téléchargement de données ;
- il y a ici une petite subtilité due au fait que nous employons une adresse sécurisée ([[http://fr.wikipedia.org/wiki/HTTPS][https]]) ;
- nous allons devoir spécifier « =method= = "wget" » pour effectuer notre tâche ;
- comme le fichier est un fichier texte (contrairement au fichier binaire précédent) nous pouvons garder la valeur par défaut de l'argument =mode= :
#+name: telechargment-de-sorting.R
#+BEGIN_SRC R :session *R* :exports code
  download.file(github,"sorting.R",method="wget")
#+END_SRC

#+RESULTS: telechargment-de-sorting.R
: 0

** Chargement du fichier de fonctions dans l'espace de travail et utilisation de =explore=
Nous chargeons les fonctions définies dans =sorting.R= de façon tout à fait banale avec la commande =source= :
#+name: chargement-sorting.R
#+BEGIN_SRC R :session *R* :exports code
  source("sorting.R")
#+END_SRC

#+RESULTS: chargement-sorting.R

Nous pouvons maintenant appeler la fonction =explore= :
#+name: explore-lD
#+BEGIN_SRC R :session *R* exports code :eval never
  explore(lD,col=c("black","grey70"))
#+END_SRC
- j'ai changé ici les couleurs utilisées par défaut ; 
- lorsque la fonction est lancée, un graphe apparaît et =R= vous fait des propositions en ligne de commande ;
- il suffit de faire un choix puis de taper =return=.  


* Normalisation des données et détection des événements :export:

** Normalisation des événements (1)
- notre première figure (\ref{fig:lD-complet}) suggère que le rapport signal sur bruit n'est pas homogène sur l'ensemble des quatre sites ;
- il semble être moins bon sur le quatrième ;
- nous allons détecter les événements sur chacun des sites et nous ne voulons pas qu'un site contribue plus ou moins à la détection à cause d'une différence de rapport signal sur bruit ;
- en somme, nous souhaitons normaliser les données de chaque site d'enregistrement de sorte que l'écart type du bruit soit le même partout ; 
- le problème est que si nous *estimons* l'écart type du bruit « directement » en ignorant les événements, la présence de ces derniers va biaiser notre estimation ;
- mais nous ne pouvons pas retirer les événements sans les avoir détecter ;
- nous souhaitons donc appliquer une méthode d'estimation de l'écart type qui ne soit pas trop affectée par la présence des événements.

** Normalisation des événements (2)
- nous souhaitons donc une méthode [[http://en.wikipedia.org/wiki/Robust_statistics][robuste]] d'estimation de l'écart type ;
- la médiane de la valeur absolue des écarts à la médiane ([[http://en.wikipedia.org/wiki/Median_absolute_deviation][Median absolute deviation]]), =MAD= = médiane(|X_i - médiane(X_j)|), fournit ce type d'estimateur : $\hat{\sigma} \approx 1.486 \, \mathrm{MAD}$ ;
- la fonction =mad= de =R= renvoie la valeur cherchée ;
- la =MAD= de chacun des quatre sites est donc simplement obtenue avec :
#+name: lD-MAD
#+BEGIN_SRC R :session *R* :exports both :cache yes
  lD.mad <- apply(lD,2,mad)
  round(lD.mad,digits=2)
#+END_SRC  

#+RESULTS[2c42f7990852031a417da1936c50a294459d57f0]: lD-MAD
| 0.52 |
| 0.63 |
| 0.74 |
| 0.68 |

** Normalisation des événements (3)

- nous allons normaliser chaque colonne en la divisant par sa =MAD= ;
- nous allons faire cela de façon « compacte », sans écrire de boucle en utilisant les propriétés de division d'une matrice par un vecteur dans =R= – attention, nous parlons ici de propriétés propres à =R=, pas de propriétés mathématiques ;
- si =M= est une matrice et =V= est un vecteur alors pour calculer =M / V=, =R= « convertit » d'abord la matrice en vecteur *en raboutant les colonnes*, les éléments « du vecteur =M= » ou ceux de =V= sont ensuite recyclés de sorte que les deux vecteurs aient la même longueur (le même nombre d'éléments) puis la division est effectuée élément par élément avant de reconvertir le résultat en une matrice ayant les mêmes dimensions que la matrice de départ =M=..

** Normalisation des événements (4)

- comme la conversion de la matrice est effectuée en raboutant les colonnes, nous devons travailler avec la transposée de =lD= pour obtenir ce que nous voulons (prenez une petite minute pour y réfléchir) ;
- lorsqu'un objet de classe suite chronologique multivariée est transposé, la classe =mts= est perdue et un objet de classe =matrix= en résulte ;
- nous utilisons finalement les deux instructions suivantes :
#+name: normalisation-lD
#+BEGIN_SRC R :session *R* :exports code :results output silent
  lD <- t(t(lD)/lD.mad)
  lD <- ts(lD,start=0,freq=15e3)
#+END_SRC

** Normalisation des événements (5)

Nous allons comparer au moyen d'un [[http://fr.wikipedia.org/wiki/Diagramme_Quantile-Quantile][diagramme Quantile-Quantile]] la normalisation basée sur la =MAD= et celle basée sur l'estimateur classique de l'écart type.
#+LaTeX: \scriptsize
#+name: comparaison-normalisation
#+BEGIN_SRC R
  gogoQ <- qnorm(ppoints(3e5))
  plot(c(-5,5),c(-5,5),type="n",
       xlab="Quantiles théoriques",
       ylab="Quantiles empiriques",
       asp=1)
  abline(v=c(-1,1)*1.25,lty=3)
  abline(a=0,b=1)
  colV <- c("red","grey50","blue","orange")
  invisible(sapply(1:4,
                   function(i) {
                     x <- lD[,i]
                     lines(gogoQ,sort(x),col=colV[i],lwd=2)
                     lines(gogoQ,sort(x/sd(x)),col=colV[i],lwd=2,lty=2)}))
#+END_SRC
#+LaTeX: \normalsize

** Normalisation des événements (6)
#+caption: Diagrammes Quantiles-Quantiles de comparaison de la normalisation basée sur la =MAD= (lignes continues) et de celle basée sur l'écart type (lignes brisées) pour chacun des quatre sites (1 en rouge, 2 en gris, 3 en bleu, 4 en orange). Les quantiles théoriques sont ceux d'une loi normale centrée réduite. Les lignes verticales pointillées coupent l'axe des x en -1,25 et +1,25.
#+attr_latex: width=0.8\textwidth
#+name: fig-comparaison-normalisation
#+header: :width 1000 :height 800 :cache yes :noweb yes
#+BEGIN_SRC R :session *R* :exports results :results output graphics :file fig-comparaison-normalisation.png
  par(cex=2)
  <<comparaison-normalisation>>
#+END_SRC

#+RESULTS[70c8a8570667ac3aa741023fb75d297de6f833fc]: fig-comparaison-normalisation
[[file:fig-comparaison-normalisation.png]]

** Détection des événements (1)
Nous allons utiliser une méthode plutôt simple pour détecter nos événements – des maxima locaux au-dessus d'un seuil :
- comme nous cherchons des maxima, nous cherchons des endroits où la dérivée première s'annule ; dit autrement, nous cherchons des endroits où la trace est localement « plate » ;
- nous nous attendons donc à ce que le bruit à haute fréquence – celui dont la valeur change significativement sur la durée caractéristique du pic d'un événement – ait une influence sur notre détection ; dit autrement, le bruit à haute fréquence devrait nous faire détecter un pic apparent un peu avant ou un peu après le vrai pic ;
- pour atténuer cet effet, nous allons « filtrer »nos données , au sens du [[http://fr.wikipedia.org/wiki/Traitement\_num\%C3\%A9rique\_du\_signal][traitement numérique du signal]], en remplaçant chaque amplitude mesurée par la moyenne d'elle même et de quelques une de ses plus proches voisines.
** Détection des événements (2)
Le filtrage que nous souhaitons mettre en œuvre, ainsi que d'autres plus sophistiqués, est simplement effectué avec la fonction =filter= de =R= :
#+name: filtre-lD
#+BEGIN_SRC R :session *R* :exports code :results output silent
  lDf <- filter(lD,rep(1,3)/3)
#+END_SRC 
Nous avons ici remplacé chaque amplitude par la moyenne d'elle-même et de ses deux plus proches voisines :
- mathématiquement notre filtre est une fonction définie sur l'ensemble des entiers, nulle partout sauf en -1, 0 et 1 où elle prend la valeur 1/3, le filtrage est un produit de convolution ;
- le filtrage est numériquement effectué par transformation de Fourier des données et du filtre, multiplication des transformées de Fourier et transformation inverse ;  
- il y a bien d'autres types de filtres pour les suites chronologiques, celui que nous avons employé donne de bons résultats en pratique sur le type de données qui nous occupe.
** Détection des événements (3)
Si vous voulez vous faire une idée de l'effet de la « longueur du filtre » – le nombre d'amplitudes voisines moyennées – vous pouvez générer deux autres versions filtrées des données en utilisant par exemple les 10 et 20 plus proche voisines, puis examiner ce que cela donne sur le premier site d'enregistrement. C'est ce que font les commandes suivantes :
#+name: effet-longueur-du-filtre
#+BEGIN_SRC R :exports code :eval never
  lDf11 <- filter(lD,rep(1,11)/11)
  lDf21 <- filter(lD,rep(1,21)/21)
  explore(cbind(lD[,1],lDf[,1],lDf11[,1],lDf21[,1]),
          col=c("black","grey70"))                
#+END_SRC

** Détection des événements (4)
Comme nous avons filtré nos trace, nous avons modifié les propriétés du bruit (c'était le but) d'une façon potentiellement dépendante du site d'enregistrement, il est donc plus sûr de renormaliser chaque trace en la divisant par sa =MAD= :
#+name: normalise-lDf 
#+BEGIN_SRC R :session *R* :exports code :results output silent
  lDf.mad <- apply(lDf,2,mad,na.rm=TRUE)
  lDf <- t(t(lDf)/lDf.mad)
#+END_SRC
Nous « rectifions » ensuite chaque trace en forçant toutes les amplitudes inférieures à un seuil de détection à zéro :
#+name: rectification-lDf
#+BEGIN_SRC R :session *R* :exports code :results output silent
  thrs <- c(4,4,4,4)
  bellow.thrs <- t(t(lDf) < thrs)
  lDfr <- lDf
  lDfr[bellow.thrs] <- 0
#+END_SRC
Ici nous employons le même seuil, quatre fois la =MAD=, sur chaque site.

** Détection des événements (5)
Nous pouvons facilement construire une figure illustrant ce que nous venons de faire sur le premier site :
#+name: plot-illustration-rectification
#+BEGIN_SRC R
  plot(window(lD[,1],0,0.2),ylab="",xlab="Temps (s)")
  abline(h=4,col=4,lty=2,lwd=2)
  lines(window(ts(lDfr[,1],start=0,freq=15e3),0,0.2),
        col=2)
#+END_SRC 

** Détection des événements (6)
#+caption: Les 200 premières millisecondes sur le premier site (noire, données brutes non filtrées) et la version rectifiée (rouge) où toutes les amplitudes inférieures à 4 fois la =MAD= (ligne horizontale en tirets bleus) ont été forcées à 0.
#+label: fig-illustration-rectification
#+attr_latex: width=0.7\textwidth
#+name: fig-illustration-rectification
#+header: :width 1000 :height 800 :noweb yes :cache yes
#+BEGIN_SRC R :session *R* :exports results :results output graphics :file fig-illustration-rectification.png
  par(cex=2)
  <<plot-illustration-rectification>>
#+END_SRC 

#+RESULTS[cfb597a6c65f1f70f2482f7dd6590159e8a28d81]: fig-illustration-rectification
[[file:fig-illustration-rectification.png]]

** Détection des événements (7)
Nous allons effectuer notre détection d'événement d'une façon « un peu simpliste » mais que l'expérience montre être efficace :
- pour détecter sur chacun des 4 sites « à la fois », nous allons former, à chaque instant, la somme des 4 traces filtrées, normalisées et rectifiées ; c.-à-d. la somme des 4 traces rouges – comme celle montrée sur la figure précédente (\ref{fig-illustration-rectification}) ;
- la commande qui effectue cette somme est =apply(lDfr,1,sum)= dans le code ci-dessous ;
- une fois cette somme formée, nous allons chercher des maxima locaux sur une fenêtre de 1 ms (15 points d'échantillonage) ;
- j'ai adapté une fonction =peaks= qui fait cela :
#+name: detection-des-PAs
#+BEGIN_SRC R :session *R* :exports code 
  posPA <- peaks(apply(lDfr,1,sum),15)
#+END_SRC

#+RESULTS: detection-des-PAs
** Détection des événements (8)
- le résultat de notre détection est stocké dans un objet, =posPA=, de classe =evtsPos= ;
- l'objet est essentiellement un vecteur d'indices – les points d'échantillonage auxquels des événements ont été détectés ;
- des informations supplémentaires, essentiellement sur le « contexte » dans lequel l'objet a été créé, sont également stockées dans celui-ci ;
- la méthode =print= appelée sur l'objet renvoie une courte description textuelle de celui-ci :
#+name: print-posPA
#+BEGIN_SRC R :session *R* :exports both :results output
  posPA
#+END_SRC 
#+LaTeX: \tiny
#+RESULTS: print-posPA
: 
: eventsPos object with indexes of 1769 events. 
:   Mean inter event interval: 169,45 sampling points, corresponding SD: 150,2 sampling points 
:   Smallest and largest inter event intervals: 9 and 1453 sampling points.
#+LaTeX: \normalsize
** Détection des événements (9)
Une fois la détection effectuée avec une méthode qui comporte autant de paramètres ajustés « à la main » :
- choix du type de filtre et de la longueur de celui-ci ;
- choix du seuil de détection ;
- choix de la longueur de la plage de détection ;
il faut *absolument* comparer données brutes et détection, ce que nous faisons ici de façon interactive avec une méthode =explore= adaptée à une combinaison d'objet de classe =eventsPos= pour le premier argument et =ts= pour le deuxième :
#+name: explore-pour-detection
#+BEGIN_SRC R :eval never
  explore(posPA, lD, col = c("black", "grey50"))
#+END_SRC
** Détection des événements (10)
#+LaTeX: \small
- il est clair lors de la confrontation de la détection aux données brutes que certains événements de petites amplitudes ne sont pas détectés ;
- la structure neuronale générant les données est telle que plus on s'éloigne des sites d'enregistrement, plus nombreuses sont les sources de signaux (les neurones) ;
- mais plus on s'éloigne, plus petites sont les amplitudes des signaux que génèrent les neurones sur les sites d'enregistrement ;
- nous nous trouvons ainsi vite dans une situation où de « nombreux » neurones distants nous génèrent de petits signaux *que nous ne pouvons plus distinguer / classer du fait du faible rapport signal sur bruit* ;
- c'est ici l'expérience des données et de la méthode d'analyse qui permet d'arriver à un choix « satisfaisant » de paramètres de détection.   
#+LaTeX: \normalsize
